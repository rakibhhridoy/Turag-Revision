import pandas as pd
import shap


doA = pd.read_csv("../data/LocationA.csv", index_col="Date")
doA.drop(columns="Unnamed: 0", inplace=True)


doA.head()


import shap
import numpy as np
import tensorflow as tf
import keras
import matplotlib.pyplot as plt

model_path = "../Multivariate/DOA/epochs100/auto_gru"
model = keras.layers.TFSMLayer(model_path, call_endpoint="serving_default")

x_test = np.load("../Multivariate/DOA/epochs100/x_test.npy")
x_train = np.load("../Multivariate/DOA/epochs100/x_train.npy")

x_train = x_train.astype(np.float32)
x_test = x_test.astype(np.float32)




import shap
import numpy as np
import tensorflow as tf
import keras
import matplotlib.pyplot as plt

model_path = "../Multivariate/DOA/epochs100/auto_gru"
model = keras.layers.TFSMLayer(model_path, call_endpoint="serving_default")

x_test = np.load("../Multivariate/DOA/epochs100/x_test.npy")
x_train = np.load("../Multivariate/DOA/epochs100/x_train.npy")



import shap
import numpy as np
import tensorflow as tf

model = tf.keras.layers.TFSMLayer("../Multivariate/DOA/epochs100/auto_gru/", call_endpoint='serving_default')

x_train = np.array(x_train).reshape(x_train.shape[0], 25, 1)
x_test = np.array(x_test).reshape(x_test.shape[0], 25, 1)

def model_predict(X):
    X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)
    output_dict = model(X_tensor)
    if isinstance(output_dict, dict):
        return list(output_dict.values())[0].numpy()
    return output_dict.numpy()

x_test_sample = x_test[:10].reshape(10, 25)

explainer = shap.KernelExplainer(lambda x: model_predict(x.reshape(x.shape[0], 25, 1)), x_train[:50].reshape(50, 25))
shap_values = explainer.shap_values(x_test_sample)

shap.summary_plot(shap_values, x_test_sample)
shap.dependence_plot(0, shap_values[0], x_test_sample)



import shap
import numpy as np
import tensorflow as tf

# Load the trained model
model = tf.keras.layers.TFSMLayer("../Multivariate/DOA/epochs100/auto_gru/", call_endpoint='serving_default')

# Reshape train and test data to match GRU input format (batch_size, time_steps, features)
x_train = np.array(x_train).reshape(x_train.shape[0], 25, 1)
x_test = np.array(x_test).reshape(x_test.shape[0], 25, 1)

# Function to predict using the model (ensuring output is in numpy format)
def model_predict(X):
    X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)
    output_dict = model(X_tensor)
    if isinstance(output_dict, dict):
        return list(output_dict.values())[0].numpy()
    return output_dict.numpy()

# Selecting a subset of test samples
x_test_sample = x_test[:10]

# Flattening input for SHAP because it expects (samples, features)
x_train_flattened = x_train.reshape(x_train.shape[0], -1)
x_test_flattened = x_test_sample.reshape(x_test_sample.shape[0], -1)

# SHAP KernelExplainer needs a function that takes 2D input, so we wrap it
explainer = shap.KernelExplainer(lambda x: model_predict(x.reshape(x.shape[0], 25, 1)), x_train_flattened[:50])

# Compute SHAP values
shap_values = explainer.shap_values(x_test_flattened)

# Convert SHAP values back to 2D for plotting
shap_values = np.array(shap_values).squeeze()

# Plot results
shap.summary_plot(shap_values, x_test_flattened)
shap.dependence_plot(0, shap_values, x_test_flattened)



shap_values.shape


import shap
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt

# Load dataset
data = pd.read_csv("../data/locationA.csv", index_col="Date")
data.drop(columns="Unnamed: 0", inplace=True)

X = data.iloc[:, :-1].astype(np.float32)
y = data.iloc[:, -1].astype(np.float32)

num_samples, num_features = X.shape
time_steps = 25  # Assumed sequence length for GRU

# Padding to ensure proper reshaping
padded_length = (num_samples // time_steps) * time_steps
X_padded = np.zeros((padded_length, num_features))
X_padded[:num_samples, :] = X.values

X_reshaped = X_padded.reshape(padded_length // time_steps, time_steps, num_features)

# Load the trained model
model = tf.keras.layers.TFSMLayer("../Multivariate/DOA/epochs100/auto_gru/", call_endpoint='serving_default')

# Model prediction function
def model_predict(X):
    X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)
    output = model(X_tensor)
    
    if isinstance(output, dict):
        first_key = list(output.keys())[0]
        return output[first_key].numpy()
    
    return output.numpy()

# Prepare SHAP input samples
x_train_sample = X_reshaped[:50]
x_test_sample = X_reshaped[:10]

x_train_flattened = x_train_sample.reshape(x_train_sample.shape[0], -1)
x_test_flattened = x_test_sample.reshape(x_test_sample.shape[0], -1)

# Initialize SHAP KernelExplainer
explainer = shap.KernelExplainer(lambda x: model_predict(x.reshape(x.shape[0], time_steps, num_features)), x_train_flattened)

# Compute SHAP values
shap_values = explainer.shap_values(x_test_flattened)
shap_values = np.array(shap_values).reshape(-1, time_steps, num_features)

# Aggregate importance per feature across time steps
feature_importance = np.abs(shap_values).mean(axis=(0, 1))

# Plot column-wise feature importance
feature_names = ['TempA', 'pHA', 'ECA', 'DOA', 'BODA', 'TNA']

plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_importance)
plt.xlabel("Mean Absolute SHAP Value")
plt.ylabel("Feature")
plt.title("Feature Importance across Time Steps")
plt.gca().invert_yaxis()
plt.show()













